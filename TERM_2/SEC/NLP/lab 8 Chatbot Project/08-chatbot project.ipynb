{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9a0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and preprocessing\n",
    "corpusFile=open('Corpus.txt','r', encoding='utf-8')\n",
    "corpus = corpusFile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8901cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to lowercase\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64f5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to list of sentences\n",
    "import nltk\n",
    "sent_tokens = nltk.sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afd65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# lemmatize the tokens\n",
    "def Lemmatize(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d9d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation dictionary\n",
    "##! The ord() function returns the number representing\n",
    "##! the unicode code of a specified character.\n",
    "\n",
    "##! The dict() function creates a dictionary.\n",
    "##! keyword arguments as much as you like, separated by comma: \n",
    "##! key = value, key = value\n",
    "import string\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563a6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and punctuations\n",
    "def normalize(text):\n",
    "    stopEnglish = set(stopwords.words('english'))\n",
    "    return Lemmatize([token for token in nltk.word_tokenize(text.lower().translate(remove_punct_dict)) if token not in stopEnglish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65e003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle of greetings\n",
    "\n",
    "import random\n",
    "##! The choice() method returns a randomly selected element\n",
    "##! from the specified sequence.\n",
    "\n",
    "def greet(sentence):\n",
    "    GREETING_INPUTS = (\"welcome\",\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "    GREETING_RESPONSES = [\"Welcome\",\"Hi\", \"Hey\", \"*nods*\", \"Hi there\", \"Hello\", \"I am glad! You are talking to me\"] \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07894297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv\n",
    "import _pickle as cPickle\n",
    "import re\n",
    "\n",
    "class EmotionAnalyser:\n",
    "    __instance = None\n",
    "\n",
    "    @staticmethod\n",
    "    def getInstance(base_path):\n",
    "        \"\"\" Static access method. \"\"\"\n",
    "        if EmotionAnalyser.__instance == None:\n",
    "            EmotionAnalyser(base_path)\n",
    "        return EmotionAnalyser.__instance\n",
    "\n",
    "    def __init__(self,base_path):\n",
    "        if EmotionAnalyser.__instance != None:\n",
    "            raise Exception(\"This class is a singleton!\")\n",
    "        else:\n",
    "            EmotionAnalyser.__instance = self\n",
    "        self.base_path = base_path\n",
    "        self.stoplist = set(stopwords.words(\"english\"))\n",
    "        self.punctuation = ['.',',','\\'','\\\"',':',';','...','-','–','—','(',')','[',']','«','»']\n",
    "        ## read more about https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual\n",
    "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) \n",
    "        # lemmatization\n",
    "        self.lemmer = nltk.stem.WordNetLemmatizer()\n",
    "        \n",
    "    \n",
    "    def extract_features(self,statement):\n",
    "        statement = statement.lower()\n",
    "        # remove all digits\n",
    "        statement = re.sub(r'[0-9]', ' ', statement)\n",
    "        # remove all (..)\n",
    "        statement = re.sub(r'\\.{2,}', ' ', statement)\n",
    "        # remove all hashtags\n",
    "        statement = re.sub(r'#.+ ', ' ', statement)        \n",
    "        # tokenization, stopwords and punctuation removal\n",
    "        word_list = [ word for word in self.tokenizer.tokenize(statement) if word not in self.stoplist and word not in string.punctuation]\n",
    "        # lemmatize the tokens\n",
    "        word_list = [self.lemmer.lemmatize(token) for token in word_list]\n",
    "        # one hot encoding\n",
    "        return dict([(word,True) for word in word_list])\n",
    "        # ngrams\n",
    "#         ngram_tubles = ngrams(word_list, 3)\n",
    "#         return dict([(gram,True) for gram in ngram_tubles])\n",
    "#         return dict([(gram,1) for gram in ngram_tubles])\n",
    "\n",
    "    def train(self):        \n",
    "        # load train data from csv file\n",
    "        csv_file = open(self.base_path+'/text_emotion.csv')\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        trainDataset = {}\n",
    "\n",
    "        for index, row in enumerate(csv_reader):\n",
    "            if index != 0:\n",
    "                if row[0] not in trainDataset:\n",
    "                    trainDataset[row[0]] = []\n",
    "                    trainDataset[row[0]].append(row[1])\n",
    "                else :\n",
    "                    trainDataset[row[0]].append(row[1])\n",
    "\n",
    "        # separate train data set into classes of emotion\n",
    "        # Split the dataset into training and testing datasets (80/20)\n",
    "        # build the train features \n",
    "        # build the test features  \n",
    "    \n",
    "        features = {}\n",
    "        thresholds = {}        \n",
    "        spilt_factor = 0.9\n",
    "        features_train = []\n",
    "        features_test = []       \n",
    "\n",
    "        for emotion in trainDataset:\n",
    "            features[emotion] = [(self.extract_features(statement), emotion) for statement in trainDataset[emotion]]\n",
    "            thresholds[emotion] = int(spilt_factor * len(features[emotion]))       \n",
    "            features_train.extend( features[emotion][:thresholds[emotion]] )\n",
    "            features_test.extend( features[emotion][thresholds[emotion]:] )\n",
    "        \n",
    "        if __name__ == \"__main__\":\n",
    "            print (\"sentiments : \",features.keys())\n",
    "            print (\"Number of training records:\", len(features_train))\n",
    "            print (\"Number of test records:\", len(features_test))\n",
    "\n",
    "        # joblib.dump(features_train, \"classifier.sav\")\n",
    "\n",
    "        # use a Naive Bayes classifier and train it\n",
    "        classifier = NaiveBayesClassifier.train(features_train)\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            print (\"Accuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
    "\n",
    "            informative = classifier.most_informative_features(1000)\n",
    "            print(informative)\n",
    "\n",
    "        # # dump classifier into a file\n",
    "        f = open(self.base_path+'/classifier.pickle', 'wb')        \n",
    "        cPickle.dump(classifier, f)\n",
    "        f.close()\n",
    "        # joblib.dump(classifier, \"classifier.save\")\n",
    " \n",
    "\n",
    "    def classify(self,statement,classifier =None): \n",
    "        if classifier == None :\n",
    "            f = open(self.base_path+'/classifier.pickle', 'rb')  \n",
    "            classifier = cPickle.load(f)\n",
    "            f.close()\n",
    "            # using joblib\n",
    "            # classifier = joblib.load(\"classifier.save\")\n",
    "        probdist = classifier.prob_classify(self.extract_features(statement))\n",
    "        predected_sentiment = probdist.max()\n",
    "        probability = round(probdist.prob(predected_sentiment), 2)\n",
    "        return predected_sentiment, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d147b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "# example of bag of n-grams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 3\n",
    "grams = ngrams(sentence.split(), n)\n",
    "\n",
    "for gram in grams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1be85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21e3bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation\n",
    "analyser = EmotionAnalyser.getInstance(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9756d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiments :  dict_keys(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise', 'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'])\n",
      "Number of training records: 35996\n",
      "Number of test records: 4004\n",
      "Accuracy of the classifier: 0.15034965034965034\n",
      "[('hate', True), ('bored', True), ('california', True), ('edited', True), ('fifteen', True), ('finishing', True), ('fault', True), ('upset', True), ('aaa', True), ('ankle', True), ('blond', True), ('brightens', True), ('bullshit', True), ('cock', True), ('confusing', True), ('cooperating', True), ('cracked', True), ('damned', True), ('declined', True), ('demon', True), ('deserve', True), ('difficult', True), ('dirt', True), ('dope', True), ('dye', True), ('ea', True), ('eee', True), ('ffs', True), ('fishing', True), ('given', True), ('grabbing', True), ('historical', True), ('idiot', True), ('illegal', True), ('lag', True), ('ligament', True), ('mental', True), ('mgmt', True), ('migraine', True), ('mud', True), ('nin', True), ('randomly', True), ('rented', True), ('sake', True), ('sean', True), ('stressing', True), ('toaster', True), ('torn', True), ('upto', True), ('wendys', True), (\"where's\", True), ('yer', True), ('suck', True), ('stupid', True), ('aaawww', True), ('accomplish', True), ('ack', True), ('awh', True), ('flag', True), ('gt', True), ('habit', True), ('jeeez', True), ('magnet', True), ('obnoxious', True), ('ohmygod', True), ('prematurely', True), ('producer', True), ('pusher', True), ('re-dyed', True), ('risk', True), ('sausage', True), ('shout', True), ('soar', True), ('switch', True), ('task', True), ('teasing', True), ('triple', True), ('uuugh', True), ('washed', True), ('yelling', True), ('boring', True), ('fuck', True), (\"mother's\", True), ('library', True), ('gah', True), ('wtf', True), ('aaand', True), ('afford', True), ('arlington', True), ('author', True), ('bea', True), ('beast', True), ('bff', True), ('bore', True), ('bothered', True), ('brake', True), ('bunny', True), ('buuut', True), ('cable', True), ('chemistry', True), ('co-worker', True), ('cs', True), ('cu', True), ('damnit', True), ('dane', True), ('delayed', True), ('deposit', True), ('dislike', True), ('distracted', True), ('ditto', True), ('fashioned', True), ('finale', True), ('focused', True), ('foundation', True), ('gigantic', True), ('hampton', True), ('helen', True), ('hung', True), ('hyper', True), ('initial', True), ('jodi', True), ('lounge', True), ('lunchhh', True), ('maintenance', True), ('mikey', True), ('mummy', True), ('nauseous', True), ('patience', True), ('pendulum', True), ('possibility', True), ('postcard', True), ('prolly', True), ('purchase', True), ('push', True), ('rc', True), ('remote', True), ('seafood', True), ('sittin', True), ('skool', True), ('sox', True), ('steve', True), ('sundae', True), ('sync', True), ('thier', True), ('utah', True), ('wifi', True), ('d:', True), ('laughing', True), ('realy', True), ('tube', True), ('mother', True), ('asian', True), ('boiling', True), ('bonfire', True), ('ca', True), ('common', True), ('counting', True), ('debating', True), ('documentation', True), ('dreadful', True), ('filling', True), ('folio', True), ('hiya', True), ('hols', True), ('hunt', True), ('irregularly', True), ('listenin', True), ('loll', True), ('looming', True), ('metaverse', True), ('nomatter', True), ('nxt', True), ('oneself', True), ('ovi', True), ('particularly', True), ('picnic', True), ('pooping', True), ('rental', True), ('shipment', True), ('size', True), ('sorting', True), ('studyin', True), ('suitcase', True), ('tarmac', True), ('teller', True), ('toll', True), ('transit', True), ('twenty', True), ('ultra', True), ('unpleasant', True), ('headache', True), ('sam', True), ('account', True), ('allowed', True), ('argument', True), ('arse', True), ('blocked', True), ('brb', True), ('cane', True), (\"chili's\", True), ('chunk', True), ('code', True), ('crap', True), ('dunno', True), ('exist', True), ('famous', True), ('ff', True), ('fully', True), ('giving', True), ('ground', True), ('guilty', True), ('kill', True), ('mcmuffin', True), ('nappy', True), ('numb', True), ('preciate', True), ('qw', True), ('respond', True), ('sleepover', True), ('slow', True), ('sunburn', True), ('traffic', True), ('turned', True), ('whin', True), ('windy', True), ('beginning', True), ('bein', True), ('brown', True), ('chest', True), ('cycle', True), ('font', True), ('friendly', True), ('helpful', True), ('jesus', True), ('legit', True), ('mcfly', True), ('parking', True), ('piss', True), ('pound', True), ('prove', True), ('rid', True), ('scotland', True), ('sheesh', True), ('sims', True), ('snack', True), ('stick', True), ('stomach', True), ('voice', True), ('wicked', True), ('accounting', True), ('bnp', True), ('breeding', True), ('downloading', True), (\"everything's\", True), ('explode', True), ('german', True), ('grow', True), ('hella', True), ('jamies', True), ('law', True), ('locked', True), ('quad', True), ('stuffed', True), ('white', True), ('anythin', True), ('blend', True), ('cooking', True), ('disgusted', True), ('drove', True), ('empire', True), ('father', True), ('jet', True), ('rabbit', True), ('sky', True), ('taste', True), ('as', True), ('love', True), ('sad', True), ('fixed', True), ('math', True), ('moving', True), ('tool', True), (\"what's\", True), ('throat', True), ('fucking', True), ('bug', True), ('finding', True), ('freezing', True), ('happening', True), ('hav', True), ('horrible', True), ('interview', True), ('middle', True), ('obsession', True), ('oven', True), ('pay', True), ('question', True), ('rd', True), ('serious', True), ('sort', True), ('touched', True), ('travelin', True), ('tune', True), ('wing', True), ('yup', True), ('annoying', True), ('butt', True), ('eww', True), ('gas', True), ('insane', True), ('lemon', True), ('plant', True), ('sec', True), ('ugh', True), ('figured', True), ('invite', True), ('loving', True), ('social', True), ('cook', True), ('worst', True), ('across', True), ('bleh', True), ('bloody', True), ('brace', True), ('britain', True), ('cancelled', True), ('deadline', True), ('deb', True), ('deep', True), ('dentist', True), ('firts', True), ('fo', True), ('grr', True), ('hitting', True), ('hows', True), ('http://myloc.me/', True), ('hulu', True), ('killer', True), ('longest', True), ('macadamia', True), ('owt', True), ('par', True), ('readin', True), ('recital', True), ('requested', True), ('self', True), ('server', True), ('setting', True), ('shut', True), ('slooow', True), ('soul', True), ('train', True), ('unless', True), ('van', True), ('wide', True), ('tired', True), ('ach', True), ('alarm', True), ('backup', True), ('badly', True), ('beloved', True), ('bite', True), ('cooky', True), ('eaten', True), ('error', True), ('gorgeous', True), ('journey', True), ('lab', True), ('lawn', True), ('legal', True), ('liking', True), ('mentioned', True), ('revising', True), ('scheduled', True), ('senior', True), ('sneak', True), ('tax', True), ('terrible', True), ('title', True), ('uni', True), ('venue', True), ('yeh', True), ('#fail', True), ('. . .', True), ('choose', True), ('constant', True), ('disgusting', True), ('essay', True), ('gpu', True), ('hd', True), ('kentucky', True), ('kiss', True), ('managed', True), ('mower', True), ('mri', True), ('munchkins', True), ('nick', True), ('pant', True), ('productive', True), ('ross', True), ('scan', True), ('stream', True), ('twice', True), ('ugg', True), ('whatchu', True), ('wifey', True), ('ben', True), ('chapter', True), ('field', True), ('framework', True), ('grim', True), ('grumble', True), ('lane', True), ('loud', True), ('moro', True), ('nj', True), ('panera', True), ('pissing', True), ('processing', True), ('wknd', True), ('credit', True), ('darn', True), ('pc', True), ('rained', True), ('slept', True), ('smh', True), ('studying', True), ('sucked', True), ('toe', True), ('copy', True), ('gear', True), ('bout', True), ('mommy', True), ('change', True), ('quick', True), ('front', True), ('goood', True), ('access', True), ('bitch', True), ('fire', True), ('sprained', True), ('glad', True), ('stuck', True), ('daddy', True), ('asshole', True), ('shit', True), ('fun', True), ('sister', True), ('seriously', True), ('congratulation', True), ('happy', True), ('fuckin', True), ('grrr', True), ('loveee', True), ('sitting', True), ('loved', True), ('proud', True), (':/', True), ('apartment', True), ('brighter', True), ('bus', True), ('channel', True), ('charging', True), ('cheap', True), ('flash', True), ('fml', True), ('gb', True), ('goin', True), ('interested', True), ('lake', True), ('lonely', True), ('magic', True), ('mall', True), ('mention', True), ('milk', True), ('neighbour', True), ('online', True), ('perhaps', True), ('pity', True), ('plz', True), ('public', True), ('puter', True), ('shift', True), ('sold', True), ('talkin', True), ('throw', True), ('toy', True), ('drunk', True), ('glass', True), ('pissed', True), ('putting', True), ('walking', True), ('card', True), ('damn', True), ('sat', True), ('wonderful', True), ('driving', True), ('high', True), ('hungry', True), ('packing', True), ('power', True), ('usual', True), ('weird', True), ('hurt', True), (':]', True), ('alot', True), ('argh', True), ('dragged', True), ('everytime', True), ('ii', True), ('jon', True), ('joy', True), (\"mom's\", True), ('bgt', True), ('left', True), ('relief', True), ('airport', True), ('saturday', True), ('con', True), ('detail', True), ('station', True), ('uncomfortable', True), ('wrist', True), ('broke', True), ('doesnt', True), ('file', True), ('half', True), ('dvd', True), ('wasnt', True), ('abroad', True), ('catch', True), ('cowell', True), ('english', True), ('join', True), ('pain', True), ('wakey', True), ('angry', True), ('bastard', True), ('wasted', True), ('lovely', True), ('hater', True), ('appreciate', True), ('finally', True), (':-p', True), ('climb', True), ('com', True), ('gem', True), ('grape', True), ('italian', True), ('liverpool', True), ('morrow', True), ('pro', True), ('score', True), ('stood', True), ('twitterville', True), ('fact', True), ('hell', True), ('tip', True), ('fine', True), ('attending', True), ('bacon', True), ('dj', True), ('escape', True), ('goodmorning', True), ('lasagna', True), ('muffin', True), ('speaking', True), ('whenever', True), ('xxx', True), ('died', True), ('internet', True), ('co', True), ('zombie', True), ('worse', True), ('posted', True), ('luv', True), ('xbox', True), ('complain', True), ('dougie', True), ('enter', True), ('heck', True), ('ily', True), ('nï', True), ('recipe', True), ('spider', True), ('barely', True), ('dm', True), ('everywhere', True), ('fail', True), ('fucked', True), ('isnt', True), ('lived', True), ('losing', True), ('storm', True), ('talent', True), ('dealership', True), ('express', True), ('jonas', True), ('ny', True), ('ughh', True), ('woman', True), ('asking', True), ('mess', True), ('aim', True), ('cleaned', True), ('clock', True), ('factory', True), ('iz', True), ('katy', True), ('kk', True), ('manager', True), ('medical', True), ('salad', True), ('sub', True), ('tennis', True), ('yu', True), ('paying', True), ('phew', True), ('positive', True), ('relaxed', True), ('safe', True), ('hated', True), ('amazing', True), ('artist', True), ('mama', True), ('cried', True), ('disappointed', True), ('thank', True), ('ppl', True), ('kinda', True), ('sending', True), (\":'(\", True), ('dead', True), ('thanks', True), ('bf', True), ('exciting', True), ('hubby', True), ('lie', True), ('order', True), ('tgif', True), ('um', True), ('million', True), ('clean', True), ('comment', True), ('foot', True), ('none', True), ('parent', True), ('timee', True), ('someday', True), ('speech', True), ('yummy', True), ('painting', True), ('theatre', True), ('uploaded', True), ('sadly', True), ('adorable', True), ('boo', True), ('nah', True), ('minute', True), ('sick', True), ('bye', True), ('link', True), ('breakfast', True), ('reading', True), ('shower', True), ('awake', True), ('cleaning', True), ('dang', True), ('iphone', True), ('past', True), ('problem', True), ('tried', True), ('catching', True), ('dealing', True), ('fed', True), ('forced', True), ('hogging', True), ('med', True), ('nut', True), ('ruined', True), ('temp', True), ('trouble', True), ('use', True), ('awesome', True), ('star', True), ('agreed', True), ('board', True), ('focus', True), ('microsoft', True), ('raining', True), ('wake', True), ('aint', True), ('boredom', True), ('dick', True), ('labor', True), ('thanx', True), ('surprise', True), ('woohoo', True), ('vista', True), ('joke', True), ('lolll', True), ('experience', True), ('midnight', True), ('wolverine', True), ('he', True), ('realize', True), ('expensive', True), ('mobile', True), ('teacher', True), ('unfair', True), ('break', True), ('hating', True), ('literally', True), ('impressed', True), ('sexy', True), ('easier', True), ('epic', True), ('humor', True), ('jimmy', True), ('yeahhh', True), ('yum', True), ('worry', True), ('cant', True), ('surprisingly', True), ('j', True), ('boyfriend', True), ('delicious', True), ('supposed', True), ('missed', True), ('computer', True), ('concert', True), ('either', True), ('fast', True), ('fones', True), ('set', True), ('talking', True), ('trolley', True), ('uk', True), ('xx', True), ('soo', True), ('beer', True), ('hang', True), ('blessed', True), ('ce', True), ('inspiration', True), ('surgery', True), ('waiting', True), ('cool', True), ('box', True), ('death', True), ('happen', True), ('husband', True), ('singing', True), ('six', True), ('whats', True), ('open', True), ('tweeting', True), ('best', True), ('accomplished', True), ('bak', True), ('cooked', True), ('portfolio', True), ('roommate', True), ('surviving', True), ('truth', True), ('anybody', True), ('lay', True), ('double', True), ('facebook', True), ('goodbye', True), ('homework', True), ('http://yfrog.com/', True), ('lazy', True), ('oops', True), ('age', True), ('chance', True), ('saying', True), ('plus', True), ('welcome', True), ('missing', True), ('hilarious', True), ('jk', True), ('relaxing', True), ('floor', True), ('sometimes', True), ('arm', True), ('car', True), ('hair', True), ('relax', True), ('sharing', True), ('degree', True), ('peep', True), ('omg', True), ('tha', True), ('happened', True), ('different', True), ('flight', True), ('sweet', True), ('fantastic', True), ('add', True), ('myspace', True), ('woo', True), ('anymore', True), ('great', True), ('load', True), ('note', True), ('round', True), ('run', True), ('shoe', True), ('slowest', True), ('received', True), ('sweetie', True), ('goodnight', True), ('hug', True), ('wow', True), ('excellent', True), ('close', True), ('definitely', True), ('e', True), ('figure', True), ('found', True), ('free', True), ('listen', True), ('london', True), ('sleepy', True), ('writing', True), ('surprised', True), ('buy', True), ('started', True), ('amazon', True), ('engine', True), ('shocked', True), ('else', True), ('enjoy', True), ('lil', True), ('point', True), ('rest', True), ('dumb', True), ('ew', True), ('mtv', True), ('pas', True), ('rubbish', True), ('ughhh', True), ('waste', True), ('camera', True), ('ohhh', True), ('posting', True), ('killing', True), ('blow', True), ('bos', True), ('geography', True), ('hold', True), ('theater', True), ('door', True), ('nasty', True), ('hoo', True), ('local', True), ('ship', True), ('wishing', True), ('band', True), ('mouse', True), ('miss', True), ('ahhh', True), ('angel', True), ('continue', True), ('dollar', True), ('drank', True), ('faith', True), ('main', True), ('nose', True), ('prob', True), ('que', True), ('queen', True), ('release', True), ('released', True), ('replying', True), ('strong', True), ('telling', True), ('#juddday', True), ('::', True), ('acceptance', True), ('ada', True), ('adium', True), ('adult', True), ('agency', True), ('agh', True), ('aid', True), ('allright', True), ('alpine', True), ('ann', True), ('anyhoo', True), ('application', True), ('bandwagon', True), ('battle', True), ('bbl', True), ('bck', True), ('bestest', True), ('blanket', True), ('blip', True), ('boost', True), ('boss', True), ('brit', True), (\"c'mon\", True), ('calorie', True), ('candy', True), ('carnival', True), ('catwalk', True), ('cent', True), ('charity', True), ('cheated', True), ('chem', True), ('chorus', True), ('completing', True), ('conan', True), ('corey', True), ('crime', True), ('crimson', True), ('ctrl', True), ('cue', True), ('dating', True), ('deangeloredman', True), ('deliver', True), ('delivered', True), ('denver', True), ('designed', True), ('diagnosis', True), ('discount', True), ('discovered', True), ('dollhouse', True), ('doubled', True), ('doubt', True), ('dressing', True), ('duuude', True), ('elk', True), ('empty', True), ('everrr', True), ('exotic', True), ('exploring', True), ('fairly', True), ('fallout', True), ('farmer', True), ('fightstar', True), ('fil', True), ('financial', True), ('florida', True), ('fudge', True), ('fusion', True), ('fï', True), ('gable', True), ('gahh', True), ('gd', True), ('georgia', True), ('girlies', True), ('gm', True), ('googled', True), ('gravity', True), ('grew', True), ('grove', True), ('hawaii', True), ('hawk', True), ('hayes', True), ('heeey', True), ('hid', True), ('hiii', True), ('hittin', True), ('hive', True), ('http://tinyurl.com/m', True), ('hyped', True), ('ian', True), ('ik', True), ('il', True), ('impatient', True), ('interest', True), ('isaac', True), ('jbs', True), ('jeff', True), ('jenn', True), ('jess', True), ('joint', True), ('knackered', True), ('knife', True), ('knock', True), ('lad', True), ('lamb', True), ('laura', True), ('lifting', True), ('logging', True), ('los', True), ('manage', True), ('medicine', True), ('mickey', True), ('minneapolis', True), ('mite', True), ('mozart', True), (\"nan's\", True), ('nc', True), ('newborn', True), ('newcastle', True), ('nicole', True), ('nlc', True)]\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "analyser.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f952def2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence : I dislike batman\n",
      "boredom with confidence : 0.39\n"
     ]
    }
   ],
   "source": [
    "# predict the sentiment with the model\n",
    "sent = input(\"Enter a sentence : \")\n",
    "# sent = \"This movie is awesome\"\n",
    "sentiment , confidence = analyser.classify(sent)\n",
    "print(sentiment , \"with confidence :\" , confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a48e1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle of emotion\n",
    "def handleEmotion(user_response,classfier = None):\n",
    "    analyser = EmotionAnalyser.getInstance(\".\")\n",
    "    sentiment , confidence = analyser.classify(user_response,classfier)        \n",
    "    possitive = ['enthusiasm','fun','happiness','love','surprise','relief']\n",
    "    negative = ['anger','boredom','hate','sadness','worry']\n",
    "    nutral = ['empty','neutral'] \n",
    "    if sentiment in nutral:\n",
    "        return \"I am sorry! I don't understand you\" , sentiment\n",
    "    if sentiment in possitive:\n",
    "        return random.choice([\"I am happy for your \"+sentiment,\"Keep up your good feelings :)\",\"Hooray!\",\"Good for you\",\"I am happy for you\"]) , sentiment\n",
    "    if sentiment in negative:\n",
    "        return random.choice([\"You aren't alone\",\"Cheer up\",\"I am sad for your \"+sentiment,\"It's ganna be okay\",\"Sorry to hear that :(\",\"It will be alright\",\"It's bad for you to feel \"+sentiment]) , sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "232f4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response from corpus to the user's questions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "\n",
    "    # add user_response to sent_tokens \n",
    "    sent_tokens.append(user_response) \n",
    "\n",
    "    # Apply TF-IDF\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=normalize)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "\n",
    "    # Apply cosine similarity to user_response and the corpus\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    print('cosine_similarity:',vals)\n",
    "    ##! argsort(): Returns the indices that would sort an array. \n",
    "    idx=vals.argsort()[0][-2]\n",
    "    print('indices that would sort:',vals.argsort())\n",
    "    ##! flatten(): Return a copy of the array collapsed into one dimension.\n",
    "    flat = vals.flatten()\n",
    "    print('flatten vals:',flat)\n",
    "    flat.sort()\n",
    "    print('flatten vals:',flat)\n",
    "    req_tfidf = flat[-2]\n",
    "    print('potential ans:',req_tfidf)\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        return \"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "        result = sent_tokens[idx]\n",
    "        sent_tokens.remove(user_response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3fa3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up the conversation along with sentiment\n",
    "def generate_reply(user_response,classfier = None):\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            return \"You are welcome..\" , \"relief\"\n",
    "        else:\n",
    "            greeting = greet(user_response)\n",
    "            if(greeting != None):\n",
    "                return greeting , \"happiness\"\n",
    "            else:     \n",
    "                result = response(user_response)                \n",
    "                if(result == \"I am sorry! I don't understand you\"):\n",
    "                    return handleEmotion(user_response,classfier)\n",
    "                else :\n",
    "                    return result , \"neutral\"\n",
    "    else:      \n",
    "        return \"Bye! take care..\" , \"relief\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f9ef1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('*nods*', 'happiness')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trial #1\n",
    "generate_reply(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcba70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0.26655127 0.         0.         0.         0.         0.\n",
      "  0.14944342 0.         0.         0.         0.         0.\n",
      "  0.         0.12386919 0.         0.         0.         0.\n",
      "  0.12894911 0.16495713 0.13348603 0.         0.1596626  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26095977 0.         0.         0.20237935\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n",
      "indices that would sort: [[24 25 26 27 28 29 30 31 33 47 34 37 38 39 40 41 42 43 44 36 45 23 10  1\n",
      "   2  3  4  5  7  8  9 21 11 46 12 14 15 16 17 13 18 20  6 22 19 35 32  0\n",
      "  48]]\n",
      "flatten vals: [0.26655127 0.         0.         0.         0.         0.\n",
      " 0.14944342 0.         0.         0.         0.         0.\n",
      " 0.         0.12386919 0.         0.         0.         0.\n",
      " 0.12894911 0.16495713 0.13348603 0.         0.1596626  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.26095977 0.         0.         0.20237935\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.        ]\n",
      "flatten vals: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12386919 0.12894911 0.13348603\n",
      " 0.14944342 0.1596626  0.16495713 0.20237935 0.26095977 0.26655127\n",
      " 1.        ]\n",
      "potential ans: 0.2665512676202362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('chatbot\\nchatbot is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"What is a chatbot?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "259c42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0.10463124 0.         0.         0.         0.         0.\n",
      "  0.41584972 0.         0.11538435 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20585627 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n",
      "indices that would sort: [[24 47 25 26 27 28 29 30 31 23 44 34 35 36 37 38 39 40 41 33 22 21 20  1\n",
      "   2  3  4  5 46  7 45  9 10 11 12 13 14 15 16 17 18 19 42 43  0  8 32  6\n",
      "  48]]\n",
      "flatten vals: [0.10463124 0.         0.         0.         0.         0.\n",
      " 0.41584972 0.         0.11538435 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.20585627 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.        ]\n",
      "flatten vals: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.10463124 0.11538435 0.20585627 0.41584972\n",
      " 1.        ]\n",
      "potential ans: 0.4158497231556903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('background of chatbot\\nin 1950, alan turing\\'s famous article \"computing machinery and intelligence\" was published, which proposed what is now called the turing test as a criterion of intelligence.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"Who is Alan Turing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6ece4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0.         0.         0.         0.         0.1282141  0.07600809\n",
      "  0.         0.         0.         0.10785829 0.         0.13831769\n",
      "  0.         0.         0.         0.13060283 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.61602487 0.21618184\n",
      "  0.         0.24649887 0.06861413 0.18103829 0.         0.\n",
      "  0.         0.07594456 0.         0.09616724 0.16614955 0.\n",
      "  1.        ]]\n",
      "indices that would sort: [[ 0 22 23 47 25 26 27 28 29 30 31 32 33 36 40 41 42 44 21 20 24 18  1  2\n",
      "   3 19  7  8 10  6 12 13 14 17 16 38 43  5 45  9  4 15 11 46 39 35 37 34\n",
      "  48]]\n",
      "flatten vals: [0.         0.         0.         0.         0.1282141  0.07600809\n",
      " 0.         0.         0.         0.10785829 0.         0.13831769\n",
      " 0.         0.         0.         0.13060283 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.61602487 0.21618184\n",
      " 0.         0.24649887 0.06861413 0.18103829 0.         0.\n",
      " 0.         0.07594456 0.         0.09616724 0.16614955 0.\n",
      " 1.        ]\n",
      "flatten vals: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.06861413\n",
      " 0.07594456 0.07600809 0.09616724 0.10785829 0.1282141  0.13060283\n",
      " 0.13831769 0.16614955 0.18103829 0.21618184 0.24649887 0.61602487\n",
      " 1.        ]\n",
      "potential ans: 0.6160248664640631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('virtual assistant\\nintelligent virtual assistant (iva) or intelligent personal assistant (ipa) is a software agent that can perform tasks or services for an individual based on verbal commands[citation needed].',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"What is intelligent virtual assistant?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e53d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 1.]]\n",
      "indices that would sort: [[ 0 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 26 25\n",
      "  24 23  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 12\n",
      "  48 49]]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1.]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1.]\n",
      "potential ans: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('i am bored', 'neutral')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"I am bored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3ceb7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]]\n",
      "indices that would sort: [[ 0 27 28 29 30 31 32 33 34 35 36 26 37 39 40 41 42 43 44 45 46 47 48 38\n",
      "  49 25 23  1  2  3  4  5  6  7  8  9 10 24 11 13 14 15 16 17 18 19 20 21\n",
      "  22 12 50]]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1.]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1.]\n",
      "potential ans: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Hooray!', 'happiness')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"I am watching this movie tomorrow night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ecebe8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Bye! take care..', 'relief')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take user query through input()\n",
    "query = input()\n",
    "generate_reply(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ed51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944b769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonnlp",
   "language": "python",
   "name": "handsonnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
