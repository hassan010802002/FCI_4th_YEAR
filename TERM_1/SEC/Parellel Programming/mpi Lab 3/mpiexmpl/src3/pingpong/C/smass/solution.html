<TITLE>A Solution to Benchmarking point to point performance with contention with MPI_Ssend</TITLE>
<BODY BGCOLOR="FFFFFF">
<PRE>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "mpi.h"

#define NUMBER_OF_TESTS 10

/* 
   This version of pingpong sends and receives from the processes
   p, p + n/2
 */

int main( argc, argv )
int argc;
char **argv;
{
    double       *buf;
    int          rank, size;
    int          n;
    double       t1, t2, tmin, tmin_local;
    int          j, k, nloop;
    MPI_Status   status;
    int          source, dest;

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Init.html#MPI_Init">MPI_Init</A>( &amp;argc, &amp;argv );

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</A>( MPI_COMM_WORLD, &amp;rank );
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</A>( MPI_COMM_WORLD, &amp;size );
    if ((size % 2) != 0) {
	if (rank == 0) 
	    printf( "Must use an even number of processes\n" );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Abort.html#MPI_Abort">MPI_Abort</A>( MPI_COMM_WORLD, 1 );
    }

    /* The rate printed by this program is the rate in the presense of contention */
    if (rank == 0) 
	printf( "Kind (np=%d)\tn\ttime (sec)\tRate (MB/sec)\n", size  );

    source = rank;
    dest   = (rank + (size/2)) % size;

    for (n=1; n&lt;1100000; n*=2) {
	if (n == 0) nloop = 1000;
	else   	    nloop  = 1000/n;
	if (nloop &lt; 1) nloop = 1;

	buf = (double *) malloc( n * sizeof(double) );
	if (!buf) {
	    fprintf( stderr, 
		     "Could not allocate send/recv buffer of size %d\n", n );
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Abort.html#MPI_Abort">MPI_Abort</A>( MPI_COMM_WORLD, 1 );
	}
	tmin = 1000;
	for (k=0; k&lt;NUMBER_OF_TESTS; k++) {
	    /* The barrier helps each link to start at about the same time */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</A>( MPI_COMM_WORLD );
	    if (source &lt; size/2) {
		/* Make sure both processes are ready */
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, dest, 14,
			      MPI_BOTTOM, 0, MPI_INT, dest, 14, MPI_COMM_WORLD, 
			      &amp;status );
		t1 = <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>();
		for (j=0; j&lt;nloop; j++) {
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Ssend.html#MPI_Ssend">MPI_Ssend</A>( buf, n, MPI_DOUBLE, dest, k, MPI_COMM_WORLD );
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( buf, n, MPI_DOUBLE, dest, k, MPI_COMM_WORLD, 
			      &amp;status );
		}
		t2 = (<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>() - t1) / nloop;
		if (t2 &lt; tmin) tmin = t2;
	    }
	    else {
		tmin = 0.0;
		/* Make sure both processes are ready */
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, dest, 14,
			      MPI_BOTTOM, 0, MPI_INT, dest, 14, MPI_COMM_WORLD, 
			      &amp;status );
		for (j=0; j&lt;nloop; j++) {
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( buf, n, MPI_DOUBLE, dest, k, MPI_COMM_WORLD, 
			      &amp;status );
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Ssend.html#MPI_Ssend">MPI_Ssend</A>( buf, n, MPI_DOUBLE, dest, k, MPI_COMM_WORLD );
		}
	    }
	}
	/* Convert to half the round-trip time */
	tmin = tmin / 2.0;
	tmin_local = tmin;
	/* Get the WORST case for output (could use MPI_MAXLOC to get
	   location as well) */
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Reduce.html#MPI_Reduce">MPI_Reduce</A>( &amp;tmin_local, &amp;tmin, 1, MPI_DOUBLE, MPI_MAX, 0, 
		       MPI_COMM_WORLD );
	if (rank == 0) {
	    double rate;
	    if (tmin &gt; 0) rate = n * sizeof(double) * 1.0e-6 /tmin;
	    else          rate = 0.0;
	    printf( "Send/Recv\t%d\t%f\t%f\n", n, tmin, rate );
	}
	free( buf );
    }

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Finalize.html#MPI_Finalize">MPI_Finalize</A>( );
    return 0;
}
</PRE>
</BODY>
