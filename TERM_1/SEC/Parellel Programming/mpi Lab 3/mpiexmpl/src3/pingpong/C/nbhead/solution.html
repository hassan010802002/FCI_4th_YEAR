<TITLE>A Solution to Benchmarking point to point performance with nonblocking operations, head-to-head</TITLE>
<BODY BGCOLOR="FFFFFF">
<PRE>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "mpi.h"

#define NUMBER_OF_TESTS 10

int main( argc, argv )
int argc;
char **argv;
{
    double       *sbuf, *rbuf;
    int          rank;
    int          n;
    double       t1, t2, tmin;
    int          i, j, k, nloop;
    MPI_Status   status, statuses[2];
    MPI_Request  r[2];

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Init.html#MPI_Init">MPI_Init</A>( &amp;argc, &amp;argv );

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</A>( MPI_COMM_WORLD, &amp;rank );
    if (rank == 0) 
	printf( "Kind\t\t\t\tn\ttime (sec)\tRate (MB/sec)\n" );

    for (n=1; n&lt;1100000; n*=2) {
	if (n == 0) nloop = 1000;
	else   	    nloop  = 1000/n;
	if (nloop &lt; 1) nloop = 1;

	sbuf = (double *) malloc( n * sizeof(double) );
	rbuf = (double *) malloc( n * sizeof(double) );
	if (!sbuf || !rbuf) {
	    fprintf( stderr, 
		     "Could not allocate send/recv buffers of size %d\n", n );
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Abort.html#MPI_Abort">MPI_Abort</A>( MPI_COMM_WORLD, 1 );
	}
	tmin = 1000;
	for (k=0; k&lt;NUMBER_OF_TESTS; k++) {
	    if (rank == 0) {
		/* Make sure both processes are ready */
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 1, 14,
			      MPI_BOTTOM, 0, MPI_INT, 1, 14, MPI_COMM_WORLD, 
			      &amp;status );
		t1 = <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>();
		for (j=0; j&lt;nloop; j++) {
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Isend.html#MPI_Isend">MPI_Isend</A>( sbuf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD, 
			       &amp;r[0] );
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</A>( rbuf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD, 
			       &amp;r[1] );
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</A>( 2, r, statuses );
		}
		t2 = (<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>() - t1) / nloop;
		if (t2 &lt; tmin) tmin = t2;
	    }
	    else if (rank == 1) {
		/* Make sure both processes are ready */
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 0, 14,
			      MPI_BOTTOM, 0, MPI_INT, 0, 14, MPI_COMM_WORLD, 
			      &amp;status );
		for (j=0; j&lt;nloop; j++) {
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</A>( rbuf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD, 
			      &amp;r[0] );
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Isend.html#MPI_Isend">MPI_Isend</A>( sbuf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD, 
			       &amp;r[1] );
		    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</A>( 2, r, statuses );
		}
	    }
	}

	/* rate is MB/sec for exchange, not an estimate of the 
	   component for each isend/irecv */
	if (rank == 0) {
	    double rate;
	    if (tmin &gt; 0) rate = 2 * n * sizeof(double) * 1.0e-6 /tmin;
	    else          rate = 0.0;
	    printf( "head-to-head Isend/Irecv\t%d\t%f\t%f\n", n, tmin, rate );
	}
	free( sbuf ); free( rbuf );
    }

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Finalize.html#MPI_Finalize">MPI_Finalize</A>( );
    return 0;
}
</PRE>
</BODY>
