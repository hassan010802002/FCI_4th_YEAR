<TITLE>A Solution to Demonstration of poor performance caused by unnecessary synchronization</TITLE>
<BODY BGCOLOR="FFFFFF">
<PRE>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "mpi.h"

/* 
 * This program demonstrates the effect of unnecessary synchronization
 * even with correct (in the buffering sense) programs
 * The output is best understood from the upshot output
 */
int main( argc, argv )
int argc;
char **argv;
{
    int nr, nc, len;
    int left, right, up, down;    /* Neighbors */
    int rank, row, col;           /* My coords */
    int size, i, loop;
    double *edgein, *edgeout;
    MPI_Request req[4];
    MPI_Status  statuses[4];

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Init.html#MPI_Init">MPI_Init</A>( &amp;argc, &amp;argv );

    /* Set defaults */
    nr = 4;
    nc = 3;
    len = 2048;

    argc--;
    argv++;
    while (argc) {
	if (strcmp(argv[0], "-nr") == 0) {
	    nr = atoi( argv[1] );
	    argc -= 2;
	    argv += 2;
	}
	else if (strcmp(argv[0], "-nc") == 0) {
	    nc = atoi( argv[1] );
	    argc -= 2;
	    argv += 2;
	}
	else if (strcmp(argv[0], "-len") == 0) {
	    len = atoi( argv[1] );
	    argc -= 2;
	    argv += 2;
	}
	else {
	    fprintf( stderr, "Unrecognized arg %s\n", argv[0] );
	    argc--; argv++;
	}
    }

    /* We could use the topology functions, but to maintain exact 
       control, we setup the mesh by hand */
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</A>( MPI_COMM_WORLD, &amp;rank );
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</A>( MPI_COMM_WORLD, &amp;size );
    if (size &lt; nr * nc) {
	fprintf( stderr, "Size of MPI_COMM_WORLD is too small\n" );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Abort.html#MPI_Abort">MPI_Abort</A>( MPI_COMM_WORLD, 1 );
	return 1;
    }
    
    row = rank % nr;
    col = rank / nr;

    left = right = up = down = MPI_PROC_NULL;
    if (row &gt; 0) up = row - 1 + col * nr;
    if (col &gt; 0) left = row + (col - 1) * nr;
    if (row &lt; nr - 1) down = row + 1 + col * nr;
    if (col &lt; nc - 1) right = row + (col + 1) * nr;

    /* Allocate and initialize the data areas */
    edgein = (double *)malloc( len * 8 * sizeof(double) );
    edgeout = edgein + 4 * len;
    
    for (i=0; i&lt;8*len; i++) edgein[i] = 1.0;

    for (loop=0; loop&lt;2; loop++) {
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</A>( MPI_COMM_WORLD );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</A>( edgein, len, MPI_DOUBLE, up, 0, MPI_COMM_WORLD, &amp;req[0] );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</A>( edgein+len, len, MPI_DOUBLE, left, 1, MPI_COMM_WORLD, 
		   &amp;req[1] );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</A>( edgein+2*len, len, MPI_DOUBLE, down, 2, MPI_COMM_WORLD, 
		   &amp;req[2] );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</A>( edgein+3*len, len, MPI_DOUBLE, right, 3, MPI_COMM_WORLD, 
		   &amp;req[3] );
	
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( edgeout, len, MPI_DOUBLE, down, 0, MPI_COMM_WORLD );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( edgeout+len, len, MPI_DOUBLE, right, 1, MPI_COMM_WORLD );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( edgeout+2*len, len, MPI_DOUBLE, up, 2, MPI_COMM_WORLD );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( edgeout+3*len, len, MPI_DOUBLE, left, 3, MPI_COMM_WORLD );
	
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</A>( 4, req, statuses );
    }

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Finalize.html#MPI_Finalize">MPI_Finalize</A>();
    return 0;
}
</PRE>
</BODY>
