<TITLE>A Solution to Comparing the performance of MPI vector datatypes</TITLE>
<BODY BGCOLOR="FFFFFF">
<PRE>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "mpi.h"

#define NUMBER_OF_TESTS 10

int main( argc, argv )
int argc;
char **argv;
{
    MPI_Datatype vec1, vec_n;
    int          blocklens[2];
    MPI_Aint     indices[2];
    MPI_Datatype old_types[2];

    double       *buf, *lbuf;
    register double *in_p, *out_p;
    int          rank;
    int          n, stride;
    double       t1, t2, tmin;
    int          i, j, k, nloop;
    MPI_Status   status;

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Init.html#MPI_Init">MPI_Init</A>( &amp;argc, &amp;argv );

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</A>( MPI_COMM_WORLD, &amp;rank );

    n	   = 1000;
    stride = 24;
    nloop  = 100000/n;

    buf = (double *) malloc( n * stride * sizeof(double) );
    if (!buf) {
	fprintf( stderr, "Could not allocate send/recv buffer of size %d\n",
		 n * stride );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Abort.html#MPI_Abort">MPI_Abort</A>( MPI_COMM_WORLD, 1 );
    }
    lbuf = (double *) malloc( n * sizeof(double) );
    if (!lbuf) {
	fprintf( stderr, "Could not allocated send/recv lbuffer of size %d\n",
		 n );
	<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Abort.html#MPI_Abort">MPI_Abort</A>( MPI_COMM_WORLD, 1 );
    }

    if (rank == 0) 
	printf( "Kind\tn\tstride\ttime (sec)\tRate (MB/sec)\n" );

    /* Use a fixed vector type */
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Type_vector.html#MPI_Type_vector">MPI_Type_vector</A>( n, 1, stride, MPI_DOUBLE, &amp;vec1 );
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Type_commit.html#MPI_Type_commit">MPI_Type_commit</A>( &amp;vec1 );

    tmin = 1000;
    for (k=0; k&lt;NUMBER_OF_TESTS; k++) {
	if (rank == 0) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 1, 14,
			  MPI_BOTTOM, 0, MPI_INT, 1, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    t1 = <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>();
	    for (j=0; j&lt;nloop; j++) {
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( buf, 1, vec1, 1, k, MPI_COMM_WORLD );
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( buf, 1, vec1, 1, k, MPI_COMM_WORLD, &amp;status );
	    }
	    t2 = (<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>() - t1) / nloop;
	    if (t2 &lt; tmin) tmin = t2;
	}
	else if (rank == 1) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 0, 14,
			  MPI_BOTTOM, 0, MPI_INT, 0, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    for (j=0; j&lt;nloop; j++) {
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( buf, 1, vec1, 0, k, MPI_COMM_WORLD, &amp;status );
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( buf, 1, vec1, 0, k, MPI_COMM_WORLD );
	    }
	}
    }
    /* Convert to half the round-trip time */
    tmin = tmin / 2.0;
    if (rank == 0) {
	printf( "Vector\t%d\t%d\t%f\t%f\n", 
		n, stride, tmin, n * sizeof(double) * 1.0e-6 / tmin );
    }
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Type_free.html#MPI_Type_free">MPI_Type_free</A>( &amp;vec1 );

    /* Use a variable vector type */
    blocklens[0] = 1;
    blocklens[1] = 1;
    indices[0]   = 0;
    indices[1]   = stride * sizeof(double);
    old_types[0] = MPI_DOUBLE;
    old_types[1] = MPI_UB;
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Type_struct.html#MPI_Type_struct">MPI_Type_struct</A>( 2, blocklens, indices, old_types, &amp;vec_n );
    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Type_commit.html#MPI_Type_commit">MPI_Type_commit</A>( &amp;vec_n );

    tmin = 1000;
    for (k=0; k&lt;NUMBER_OF_TESTS; k++) {
	if (rank == 0) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 1, 14,
			  MPI_BOTTOM, 0, MPI_INT, 1, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    t1 = <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>();
	    for (j=0; j&lt;nloop; j++) {
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( buf, n, vec_n, 1, k, MPI_COMM_WORLD );
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( buf, n, vec_n, 1, k, MPI_COMM_WORLD, &amp;status );
	    }
	    t2 = (<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>() - t1) / nloop;
	    if (t2 &lt; tmin) tmin = t2;
	}
	else if (rank == 1) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 0, 14,
			  MPI_BOTTOM, 0, MPI_INT, 0, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    for (j=0; j&lt;nloop; j++) {
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( buf, n, vec_n, 0, k, MPI_COMM_WORLD, &amp;status );
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( buf, n, vec_n, 0, k, MPI_COMM_WORLD );
	    }
	}
    }
    /* Convert to half the round-trip time */
    tmin = tmin / 2.0;
    if (rank == 0) {
	printf( "Struct\t%d\t%d\t%f\t%f\n", 
		n, stride, tmin, n * sizeof(double) * 1.0e-6 / tmin );
    }

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Type_free.html#MPI_Type_free">MPI_Type_free</A>( &amp;vec_n );

    /* Use user-packing with known stride */
    tmin = 1000;
    for (k=0; k&lt;NUMBER_OF_TESTS; k++) {
	if (rank == 0) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 1, 14,
			  MPI_BOTTOM, 0, MPI_INT, 1, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    t1 = <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>();
	    for (j=0; j&lt;nloop; j++) {
		/* If the compiler isn't good at unrolling and changing
		   multiplication to indexing, this won't be as good as
		   it could be */
		for (i=0; i&lt;n; i++) 
		    lbuf[i] = buf[i*stride];
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( lbuf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD );
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( lbuf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD, &amp;status );
		for (i=0; i&lt;n; i++) 
		    buf[i*stride] = lbuf[i];
	    }
	    t2 = (<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>() - t1) / nloop;
	    if (t2 &lt; tmin) tmin = t2;
	}
	else if (rank == 1) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 0, 14,
			  MPI_BOTTOM, 0, MPI_INT, 0, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    for (j=0; j&lt;nloop; j++) {
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( lbuf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD, &amp;status );
		for (i=0; i&lt;n; i++) 
		    buf[i*stride] = lbuf[i];
		for (i=0; i&lt;n; i++) 
		    lbuf[i] = buf[i*stride];
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( lbuf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD );
	    }
	}
    }
    /* Convert to half the round-trip time */
    tmin = tmin / 2.0;
    if (rank == 0) {
	printf( "User\t%d\t%d\t%f\t%f\n", 
		n, stride, tmin, n * sizeof(double) * 1.0e-6 / tmin );
    }

    /* Use user-packing with known stride, using addition in the user
       copy code */
    tmin = 1000;
    for (k=0; k&lt;NUMBER_OF_TESTS; k++) {
	if (rank == 0) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 1, 14,
			  MPI_BOTTOM, 0, MPI_INT, 1, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    t1 = <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>();
	    for (j=0; j&lt;nloop; j++) {
		/* If the compiler isn't good at unrolling and changing
		   multiplication to indexing, this won't be as good as
		   it could be */
		in_p = buf; out_p = lbuf;
		for (i=0; i&lt;n; i++) {
		    out_p[i] = *in_p; in_p += stride;
		}
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( lbuf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD );
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( lbuf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD, &amp;status );
		out_p = buf; in_p = lbuf;
		for (i=0; i&lt;n; i++) {
		    *out_p = in_p[i]; out_p += stride;
		}
	    }
	    t2 = (<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Wtime.html#MPI_Wtime">MPI_Wtime</A>() - t1) / nloop;
	    if (t2 &lt; tmin) tmin = t2;
	}
	else if (rank == 1) {
	    /* Make sure both processes are ready */
	    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Sendrecv.html#MPI_Sendrecv">MPI_Sendrecv</A>( MPI_BOTTOM, 0, MPI_INT, 0, 14,
			  MPI_BOTTOM, 0, MPI_INT, 0, 14, MPI_COMM_WORLD, 
			  &amp;status );
	    for (j=0; j&lt;nloop; j++) {
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</A>( lbuf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD, &amp;status );
		in_p = lbuf; out_p = buf;
		for (i=0; i&lt;n; i++) {
		    *out_p = in_p[i]; out_p += stride;
		}
		out_p = lbuf; in_p = buf;
		for (i=0; i&lt;n; i++) {
		    out_p[i] = *in_p; in_p += stride;
		}
		<A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Send.html#MPI_Send">MPI_Send</A>( lbuf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD );
	    }
	}
    }
    /* Convert to half the round-trip time */
    tmin = tmin / 2.0;
    if (rank == 0) {
	printf( "User(add)\t%d\t%d\t%f\t%f\n", 
		n, stride, tmin, n * sizeof(double) * 1.0e-6 / tmin );
    }

    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Finalize.html#MPI_Finalize">MPI_Finalize</A>( );
    return 0;
}
</PRE>
</BODY>
